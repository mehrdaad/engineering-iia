{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3F7 Information Theory: Handout 1: Questions\n",
    "\n",
    "#### Key questions to discuss in this course\n",
    "1. How do you quantify information using entropy?\n",
    "    - What is entropy?\n",
    "2. What is the fundamental limit of data compression?\n",
    "3. What is the fundamental limit of reliable data transmission?\n",
    "    - Qs 2 and 3 are two sides of the same coin\n",
    "\n",
    "## Introduction\n",
    "1. Describe (in the abstract) a digital end-to-end communication system to transmit e.g. video.\n",
    "    - 1. Source coding module (compressor, decompressor)\n",
    "    - 2. Channel coding module (transmitter, channel and receiver). Doesn't care where bits came from.\n",
    "    - Flow: \n",
    "        - Image goes into compressor. \n",
    "        - Stream of bits conveyed from compressor to transmitter. \n",
    "        - Transmitter converts stream of bits into input waveform, which goes into the channel. \n",
    "        - We have no control over the channel. It distorts the input waveform, adds noise.\n",
    "        - Channel conveys output waveform to receiver.\n",
    "        - Receiver tries tor recover bits from output waveform.\n",
    "        - Receiever conveys bits to decompressor.\n",
    "        - Decompressor decompresses bits and conveys output image.\n",
    "\n",
    "## Probability Review\n",
    "\n",
    "1. What is a random variable?\n",
    "2. Define the following for a discrete random variable X:\n",
    "    - E(X), Var(X), Var(aX) for constant a, E(g(X))\n",
    "3. Define the following for jointly distributed discrete random variables:\n",
    "    - Marginal distributions, conditional distributions, the product rule (deconstruct the joint), the sum rule (marginals)\n",
    "4. Define the following for jointly distributed continuous random variables:\n",
    "    - Joint density function, joint PDF\n",
    "    - Bivariate normal (with mean vector $\\mu$ and covariance matrix $\\Sigma$.\n",
    "\n",
    "## Information Theory Fundamentals\n",
    "\n",
    "1. What is entropy? Why is it useful?\n",
    "    - Entropy $H(X) = \\sum_x P(x)\\log\\frac{1}{P(X)}$ bits\n",
    "    - It is the uncertainty associated with the random variable X.\n",
    "    - We have $H(X) = E[\\log\\frac{1}{P(X)}]$\n",
    "2. What are the properties of entropy? Prove these properties.\n",
    "    - $H(X)\\geq 0$\n",
    "    - $H(X) - \\log|\\chi| \\geq 0$\n",
    "        - Prove using log rules and $\\ln(x) \\leq x - 1$\n",
    "            - Prove the latter by looking at the derivative of $\\ln(x)$.\n",
    "3. What is the entropy of a Bernouilli random variable (i.e. the binary entropy function)?\n",
    "    - $H_2(p) = p\\log\\frac{1}{p} + (1-p)\\log\\frac{1}{1-p}$.\n",
    "    - Bernouilli Random Variable: X = 1 with probability p, X = 0 with probability 1-p.\n",
    "    - $H_2(p)$ is symmetric, concave and has a maximum at p=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
